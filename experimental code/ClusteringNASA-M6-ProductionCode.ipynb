{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DBSCAN to reduce the size of the spatial dataset\n",
    "\n",
    "#### DBScan - https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "\n",
    "#### Nearest Neighbors - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html\n",
    "\n",
    "#### Overview of DB Scan - https://medium.com/@agarwalvibhor84/lets-cluster-data-points-using-dbscan-278c5459bee5\n",
    "\n",
    "#### DBScan Article for code below - https://geoffboeing.com/2014/08/clustering-to-reduce-spatial-data-set-size/\n",
    "\n",
    "#### Notebook from above article - https://github.com/gboeing/2014-summer-travels/blob/master/clustering-scikitlearn.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------\n",
    "## Active fire data collected by NASA MODIS satellite\n",
    "\n",
    "#### MODIS Collection 6: Temporal Coverage: 2003 - 2019\n",
    "#### Data access: https://earthdata.nasa.gov/earth-observation-data/near-real-time/firms/c6-mcd14dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, time\n",
    "import datetime\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from geopy.distance import great_circle\n",
    "from shapely.geometry import MultiPoint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file into dataframe\n",
    "M6df = pd.read_csv('/Users/nahidmacbook/Documents/DataScience/Data-Wildfire/fire_archive_M6_110066.csv')\n",
    "\n",
    "# Add new columns: month & year\n",
    "M6df['month'] = pd.DatetimeIndex(M6df['acq_date']).month\n",
    "M6df['year'] = pd.DatetimeIndex(M6df['acq_date']).year\n",
    "M6df['DOY'] = pd.DatetimeIndex(M6df['acq_date']).dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to encode DOY as a cyclical feature\n",
    "def encode(data, col, max_val):\n",
    "    data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n",
    "    data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode DOY of M6df\n",
    "M6df = encode(M6df, 'DOY', 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the geographical data points to the long/lat of the United States\n",
    "M6df = M6df[(M6df.longitude > -161) & (M6df.longitude < -68) & (M6df.latitude > 19) & (M6df.latitude <65)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#C71585\"> 1- Split data into yearly datasets and run the clustering function for the year </font>\n",
    "### 1- Split data into yearly datasets and run the clustering function for the year\n",
    "- to enhance the performance of the DBSCAN module, feed the data one year at a time\n",
    "- execute steps 1-4 for years 2003 through 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# use this code if running the notebook from the terminal\n",
    "import os\n",
    "env_var = os.environ.get(ENV_VAR, 2020)\n",
    "year = env_var\n",
    "'''\n",
    "year = 2011\n",
    "M6df_yearly = M6df[(M6df.year == year)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3956\n"
     ]
    }
   ],
   "source": [
    "kms_per_radian = 3956\n",
    "### 3956 for miles, 6371.0088 for kilometers\n",
    "print(kms_per_radian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"#C71585\"> Compute DBSCAN </font>\n",
    "#### Compute DBSCAN\n",
    "\n",
    "- eps is the physical distance from each point that forms its neighborhood\n",
    "- min_samples is the min cluster size, otherwise it's noise - set to 1 so we get no noise\n",
    "\n",
    "- Extract the lat, lon columns into a numpy matrix of coordinates, then convert to radians when you call fit, for use by scikit-learn's haversine metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nahidmacbook/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "coords = M6df_yearly.as_matrix(columns=['latitude', 'longitude'])\n",
    "epsilon = 1.5 / kms_per_radian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"#C71585\"> Defining the Clustering Functions </font>\n",
    "#### Defining the Clustering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "db = DBSCAN(eps=epsilon, min_samples=1, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))\n",
    "cluster_labels = db.labels_\n",
    "\n",
    "# get the number of clusters\n",
    "num_clusters = len(set(cluster_labels))\n",
    "\n",
    "# all done, print the outcome\n",
    "message = 'Clustered {:,} points down to {:,} clusters, for {:.1f}% compression in {:,.2f} seconds'\n",
    "print(message.format(len(M6df_yearly), num_clusters, 100*(1 - float(num_clusters) / len(M6df_yearly)), time.time()-start_time))\n",
    "print('Silhouette coefficient: {:0.03f}'.format(metrics.silhouette_score(coords, cluster_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the clusters into a pandas series, where each element is a cluster of points\n",
    "clusters = pd.Series([coords[cluster_labels==n] for n in range(num_clusters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Add this line of code if min_sample > 1: \n",
    "clusters.pop(num_clusters-1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#C71585\"> 2- Move the clustered datapoints into a dataframe </font>\n",
    "### 2- Move the clustered datapoints into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loop to pull in the cluster points that make up each Cluster represented in the Series 'Clusters'.\n",
    "def preparecluster(row, clusterseries):\n",
    "    clusterDF = pd.DataFrame(clusterseries[0]) \n",
    "    clusterDF = clusterDF.assign(ClusterNum=row) \n",
    "    clusterDF = clusterDF.assign(Year=year)\n",
    "    clusterDF = clusterDF.assign(Cluster_Reference= str(row)+'_'+str(year))\n",
    "    row = 1\n",
    "    while row < len(clusterseries):\n",
    "        clusterdata = (clusterseries[row])\n",
    "        clusterDFTemp = pd.DataFrame(clusterdata)\n",
    "        clusterDFTemp = clusterDFTemp.assign(ClusterNum=row) \n",
    "        clusterDFTemp = clusterDFTemp.assign(Year=year) \n",
    "        clusterDFTemp = clusterDFTemp.assign(Cluster_Reference= str(row)+'_'+str(year))\n",
    "        clusterDF = clusterDF.append(clusterDFTemp, ignore_index=True)\n",
    "        row = row + 1\n",
    "    return clusterDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Viewing results of dataframe consolidation\n",
    "groupedclusters = preparecluster(0,clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {groupedclusters.columns[0]:'Latitude', groupedclusters.columns[1]:'Longitude', groupedclusters.columns[2]:'DOY', groupedclusters.columns[3]:'ClusterNum',groupedclusters.columns[4]:'Year',groupedclusters.columns[5]:'Cluster_Reference'}\n",
    "groupedclusters = groupedclusters.rename(columns=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the clusters dataframe into a csv\n",
    "groupedclusters.to_csv('/Users/nahidmacbook/Documents/DataScience/Data-Wildfire/TEST-ClusteredNASA-M6-'+str(year)+'.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#C71585\"> 3- Find the point in each cluster that is closest to its centroid </font>\n",
    "### 3- Find the point in each cluster that is closest to its centroid\n",
    "\n",
    "DBSCAN clusters may be non-convex. This technique just returns one representative point from each cluster. First get the lat,lon coordinates of the cluster's centroid (shapely represents the first coordinate in the tuple as x and the second as y, so lat is x and lon is y here). Then find the member of the cluster with the smallest great circle distance to the centroid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_centermost_point(cluster):\n",
    "    centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "    centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "    return tuple(centermost_point)\n",
    "\n",
    "centermost_points = clusters.map(get_centermost_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the list of centermost points (lat, lon) tuples into separate latitude and longitude lists\n",
    "latitude, longitude = zip(*centermost_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from these lats/lons create a new df of one representative point for each cluster\n",
    "rep_points = pd.DataFrame({'longitude':longitude, 'latitude':latitude})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#C71585\"> 4- Save the Cluster Points and their respective data fields into CSV </font>\n",
    "### 4- Save the Cluster Points and their respective data fields into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull row from original data set where lat/lon match the lat/lon of each row of representative points\n",
    "# that way we get the full details like frp, brightness, and date from the original dataframe\n",
    "\n",
    "rs = rep_points.apply(lambda row: M6df_yearly[(M6df_yearly['latitude']==row['latitude']) & (M6df_yearly['longitude']==row['longitude'])].iloc[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign cluster_reference to the dataframe\n",
    "rs['cluster_reference'] = rs.index.map(str)+'_'+rs['year'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.to_csv('/Users/nahidmacbook/Documents/DataScience/Data-Wildfire/TEST-NASA-M6-DBSCAN-Clusters'+ str(year) +'.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the final reduced set of coordinate points vs the original full set\n",
    "fig, ax = plt.subplots(figsize=[10, 6])\n",
    "df_scatter = ax.scatter(M6df_yearly['longitude'], M6df_yearly['latitude'], c='#FB7153', alpha=0.2, s=100)\n",
    "rs_scatter = ax.scatter(rs['longitude'], rs['latitude'], c='k', edgecolor='None', alpha=1.0, s=3)\n",
    "ax.set_title('Yearly NASA Active Fires - Full data set vs DBSCAN reduced set')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.legend([df_scatter, rs_scatter], ['Full set', 'Reduced set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

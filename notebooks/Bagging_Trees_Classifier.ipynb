{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and ExtraTrees Classifier Wild Fire Model Evaluation\n",
    "\n",
    "This notebooks will tune and run Bagging and Extra Trees classifier on wildfire dataset in order to make a predecction on fire intensity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#optional code if warnings become over bearing \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.23.1.\n"
     ]
    }
   ],
   "source": [
    "#optional code to ensure everyone is on the right version\n",
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional code if warnings become over bearing \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all the models and libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression, SGDClassifier\n",
    "\n",
    "\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn import under_sampling, over_sampling\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function to save estimator\n",
    "Saves to current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_estimator (estimator) :\n",
    "          \n",
    "    outpath = estimator.__class__.__name__.lower().replace(\" \", \"-\") + \".pickle\"\n",
    "    with open(outpath, 'wb') as f:\n",
    "        pickle.dump(estimator, f)\n",
    "        f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_estimator(filename) :\n",
    "    \n",
    "    infile = open(filename,'rb')\n",
    "    estimator = pickle.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "    print (\"Unpickled file\", filename)\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Fire Data From Disk to Save Time.\n",
    "During the feature evaluation the data was pulled from the database and stored locally\n",
    "Read a random sample.\n",
    "\n",
    "Optimal testing data is 150K - SMOTE will generate additonal data for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 26)\n"
     ]
    }
   ],
   "source": [
    "df_fires = pd.read_csv('data/FireIntensity_Model_June12_Clean.csv').sample(50000)\n",
    "\n",
    "print(df_fires.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess data in the file. \n",
    "Check data to ensure it's correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding number of columns:\n",
    "pd.set_option('display.max_columns', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>doy</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>cluster_reference</th>\n",
       "      <th>fire_region</th>\n",
       "      <th>season</th>\n",
       "      <th>covertype</th>\n",
       "      <th>fuelcode</th>\n",
       "      <th>prefire_fuel</th>\n",
       "      <th>fuel_moisture_class</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>precip_intensity</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_gust</th>\n",
       "      <th>brightness</th>\n",
       "      <th>fire_intensity</th>\n",
       "      <th>fire_intensity_twocat</th>\n",
       "      <th>fire_intensity_threecat</th>\n",
       "      <th>fire_intensity_fourcat</th>\n",
       "      <th>bright_t31</th>\n",
       "      <th>frp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1008143</th>\n",
       "      <td>1008143</td>\n",
       "      <td>45.9030</td>\n",
       "      <td>-114.8757</td>\n",
       "      <td>251</td>\n",
       "      <td>9</td>\n",
       "      <td>2012</td>\n",
       "      <td>2012_616</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1200</td>\n",
       "      <td>6424.431099</td>\n",
       "      <td>1</td>\n",
       "      <td>67.52</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.216293</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.94</td>\n",
       "      <td>312.1</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>286.5</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210079</th>\n",
       "      <td>1210079</td>\n",
       "      <td>32.2142</td>\n",
       "      <td>-85.1297</td>\n",
       "      <td>296</td>\n",
       "      <td>10</td>\n",
       "      <td>2014</td>\n",
       "      <td>2014_6000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1160</td>\n",
       "      <td>7020.760516</td>\n",
       "      <td>2</td>\n",
       "      <td>67.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.997000</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2.29</td>\n",
       "      <td>327.1</td>\n",
       "      <td>High</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>295.9</td>\n",
       "      <td>112.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103880</th>\n",
       "      <td>103880</td>\n",
       "      <td>33.1059</td>\n",
       "      <td>-79.6694</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>2004</td>\n",
       "      <td>2004_87</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1160</td>\n",
       "      <td>7020.760516</td>\n",
       "      <td>2</td>\n",
       "      <td>74.50</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.997000</td>\n",
       "      <td>6.30</td>\n",
       "      <td>12.21</td>\n",
       "      <td>343.0</td>\n",
       "      <td>Severe</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>277.2</td>\n",
       "      <td>49.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  latitude  longitude  doy  month  year cluster_reference  \\\n",
       "1008143     1008143   45.9030  -114.8757  251      9  2012          2012_616   \n",
       "1210079     1210079   32.2142   -85.1297  296     10  2014         2014_6000   \n",
       "103880       103880   33.1059   -79.6694   62      3  2004           2004_87   \n",
       "\n",
       "         fire_region  season  covertype  fuelcode  prefire_fuel  \\\n",
       "1008143            1       1          3      1200   6424.431099   \n",
       "1210079            2       2          3      1160   7020.760516   \n",
       "103880             2       3          3      1160   7020.760516   \n",
       "\n",
       "         fuel_moisture_class  temperature  humidity  precip_intensity  \\\n",
       "1008143                    1        67.52      0.36               0.0   \n",
       "1210079                    2        67.48      0.52               0.0   \n",
       "103880                     2        74.50      0.55               0.0   \n",
       "\n",
       "         visibility  wind_speed  wind_gust  brightness fire_intensity  \\\n",
       "1008143    9.216293        0.04       4.94       312.1         Medium   \n",
       "1210079    9.997000        0.19       2.29       327.1           High   \n",
       "103880     9.997000        6.30      12.21       343.0         Severe   \n",
       "\n",
       "        fire_intensity_twocat fire_intensity_threecat fire_intensity_fourcat  \\\n",
       "1008143              Moderate                     Low                    Low   \n",
       "1210079              Moderate                     Low                    Low   \n",
       "103880               Moderate                     Low                    Low   \n",
       "\n",
       "         bright_t31    frp  \n",
       "1008143       286.5   27.1  \n",
       "1210079       295.9  112.9  \n",
       "103880        277.2   49.4  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review datagram\n",
    "print (df_fires.shape)\n",
    "df_fires.head(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the columns not needed identified during the feature seleciton phase\n",
    "Recommend Features:  'temperature', 'wind_speed','humidity', 'fire_region', 'season', \n",
    "                 'fuelcode', 'prefire_fuel', 'fuel_moisture_class',\n",
    "                 'visibility', 'precip_intensity', 'wind_gust'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fires = df_fires.drop(columns={'Unnamed: 0', 'latitude', 'longitude','cluster_reference', 'doy', \n",
    "                                  'month', 'year', 'brightness', 'bright_t31','frp', 'visibility', 'covertype',\n",
    "                                  'fire_intensity_twocat', 'fire_intensity', 'fire_intensity_threecat'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'latitude', 'longitude', 'doy', 'month', 'year',\n",
       "       'cluster_reference', 'fire_region', 'season', 'covertype', 'fuelcode',\n",
       "       'prefire_fuel', 'fuel_moisture_class', 'temperature', 'humidity',\n",
       "       'precip_intensity', 'visibility', 'wind_speed', 'wind_gust',\n",
       "       'brightness', 'fire_intensity', 'fire_intensity_twocat',\n",
       "       'fire_intensity_threecat', 'fire_intensity_fourcat', 'bright_t31',\n",
       "       'frp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fires.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low         45553\n",
      "Moderate     3814\n",
      "High          527\n",
      "Severe        106\n",
      "Name: fire_intensity_fourcat, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#show the shape of the data\n",
    "print(df_fires['fire_intensity_fourcat'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the features for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['fire_region', 'season', 'fuelcode', 'fuel_moisture_class', 'prefire_fuel', 'temperature', 'humidity', 'precip_intensity', \n",
    "             'wind_gust', 'wind_speed']\n",
    "\n",
    "cat_features = ['fire_region', 'season', 'fuelcode', 'fuel_moisture_class']\n",
    "\n",
    "num_features = ['prefire_fuel', 'temperature', 'humidity', 'precip_intensity', 'wind_gust', 'wind_speed']\n",
    "\n",
    "target = ['fire_intensity_fourcat'] # four categories using balanced binning\n",
    "\n",
    "y = df_fires[target]\n",
    "y = np.ravel(y)\n",
    "X = df_fires[features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the columns transfomer for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))])\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler()),\n",
    "    ('imputer', SimpleImputer(strategy='median'))])\n",
    "\n",
    "# Setting the transformation for the pipeline below. \n",
    "preprocessor = ColumnTransformer (transformers=[\n",
    "        ('num_features', numeric_transformer, num_features),\n",
    "        ('cat_features', categorical_transformer, cat_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the models to test\n",
    "<b>Down to our best 2 models</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [     \n",
    "            ExtraTreesClassifier(n_estimators=5),\n",
    "            #BaggingClassifier(n_estimators=3),            \n",
    "        ]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Fit and predict the scores of the model. \n",
    "Give us our final scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(X, y, estimator, **kwargs):\n",
    "    \n",
    "    \n",
    "    # Processing y.\n",
    "    y = LabelEncoder().fit_transform(y) # Label-encode targets before modeling\n",
    "    \n",
    "   \n",
    "    #smote models for balance bins for classifier \n",
    "    oversample = SMOTE()\n",
    "    X, y = oversample.fit_resample(X, y)\n",
    "   \n",
    "    model = Pipeline(steps=[\n",
    "          ('preprocessor', preprocessor),\n",
    "          ('estimator', estimator)\n",
    "      ])\n",
    "    \n",
    "    # Instantiate the classification model and visualizer\n",
    "    model.fit(X, y, **kwargs)  \n",
    "    \n",
    "    expected = y\n",
    "    predicted = model.predict(X)\n",
    "    \n",
    "   \n",
    "    # Compute and return F1 (harmonic mean of precision and recall)\n",
    "    print(\"{}: {}\".format(estimator.__class__.__name__, f1_score(expected, predicted, average='micro')))\n",
    "    \n",
    "   \n",
    "    return save_estimator(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    score_model(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to visualize the model out using yellow brick classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(X, y, estimator):\n",
    "        \n",
    "    \n",
    "    # Processing y.\n",
    "    y = LabelEncoder().fit_transform(y) # Label-encode targets before modeling\n",
    "    \n",
    "   \n",
    "    #smote models for balance bins for classifier \n",
    "    oversample = SMOTE()\n",
    "    X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('estimator', estimator)\n",
    "    ])\n",
    "   \n",
    "\n",
    "    X_train, X_test, y_train, y_test = tts(X, y, test_size=0.33)\n",
    "\n",
    "    # Instantiate the classification model and visualizer\n",
    "    visualizer = ClassificationReport(\n",
    "        model,\n",
    "        classes=['Low', 'Medium', 'High', 'Severe'], # Classes for equal balanced bins per quartiles\n",
    "        cmap=\"Reds\", \n",
    "        support=True,\n",
    "        size=(800, 660)\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    visualizer.fit(X_train, y_train)  \n",
    "        \n",
    "    visualizer.score(X_test, y_test)\n",
    " \n",
    "    \n",
    "    #optional\n",
    "    #visualizer.show(outpath=estimator.__class__.__name__ + \".png\")  \n",
    "    \n",
    "    visualizer.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through the models to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    visualize_model(X, y, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_report (X, y, estimator, **kwargs):\n",
    "    \n",
    "    \n",
    "    # Processing y.\n",
    "    y = LabelEncoder().fit_transform(y) # Label-encode targets before modeling\n",
    "    \n",
    "   \n",
    "    #smote models for balance bins for classifier \n",
    "    oversample = SMOTE()\n",
    "    X, y = oversample.fit_resample(X, y)\n",
    "   \n",
    "    model = Pipeline(steps=[\n",
    "          ('preprocessor', preprocessor),\n",
    "          ('estimator', estimator)\n",
    "      ])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = tts(X, y, test_size=0.33)\n",
    "    \n",
    "    # Instantiate the classification model and visualizer\n",
    "    model.fit(X_train, y_train, **kwargs)  \n",
    "    \n",
    "    expected =  y_test\n",
    "    predicted = model.predict(X_test)\n",
    "    \n",
    "    print(confusion_matrix(expected, predicted))\n",
    "    print(classification_report(expected, predicted))\n",
    "\n",
    "    # Compute and return F1 (harmonic mean of precision and recall)\n",
    "    print(\"{}: {}\".format(estimator.__class__.__name__, f1_score(expected, predicted, average='micro')))\n",
    "\n",
    "   \n",
    "    return save_estimator(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    confusion_matrix_report(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ROCAUC For Bagging Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "visualizer = ROCAUC(BaggingClassifier(), size=(1080, 720))\n",
    "                                           \n",
    "# Create the train and test data\n",
    "X_train, X_test, y_train, y_test = tts(X, y, test_size=0.3)\n",
    "\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show()                 # Draw the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROCAUC Curve for Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classification model and visualizer\n",
    "visualizer = ROCAUC(ExtraTreesClassifier(n_estimators=10), classes=classes, size=(1080, 720))\n",
    "                                           \n",
    "# Create the train and test data\n",
    "X_train, X_test, y_train, y_test = tts(X, y, test_size=0.3)\n",
    "\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show()                 # Draw the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Class Prediction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def class_pred_error (X,y, estimator):\n",
    "\n",
    "\n",
    "    classes=['Low', 'Medium', 'High', 'Severe']\n",
    "\n",
    "    \n",
    "    oversample = SMOTE()\n",
    "    X, y = oversample.fit_resample(X, y)\n",
    "   \n",
    "    model = Pipeline(steps=[\n",
    "          ('preprocessor', preprocessor),\n",
    "          ('estimator', estimator)\n",
    "      ])\n",
    "    \n",
    "    # Perform 80/20 training/test split\n",
    "    X_train, X_test, y_train, y_test = tts(\n",
    "        X, y, test_size=0.20, random_state=42\n",
    "    )\n",
    "\n",
    "    # Instantiate the classification model and visualizer\n",
    "    '''\n",
    "    visualizer = ClassPredictionError(\n",
    "    RandomForestClassifier(n_estimators=10), \n",
    "    classes=classes, size=(1080, 720)\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    visualizer = ClassPredictionError(\n",
    "        model, \n",
    "        classes=classes, size=(1080, 720)\n",
    "    )\n",
    "    model.fit(X,y)\n",
    "\n",
    "    # Fit the training data to the visualizer\n",
    "    visualizer.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    visualizer.score(X_test, y_test)\n",
    "\n",
    "    # Draw visualization\n",
    "    visualizer.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    class_pred_error(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final F1 Cross Validated Scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_Cross_Validated_Score (X, y, estimator):\n",
    "\n",
    "\n",
    "    classes=['Low', 'Medium', 'High', 'Severe']\n",
    "\n",
    "    \n",
    "    oversample = SMOTE()\n",
    "    X, y = oversample.fit_resample(X, y)\n",
    "   \n",
    "    model = Pipeline(steps=[\n",
    "          ('preprocessor', preprocessor),\n",
    "          ('estimator', estimator)\n",
    "      ])\n",
    "    \n",
    "    \n",
    "    visualizer = ClassPredictionError(\n",
    "        model, \n",
    "        classes=classes, size=(1080, 720)\n",
    "    )\n",
    "    model.fit(X,y)\n",
    "\n",
    "    # Create a cross-validation strategy\n",
    "    cv = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "    visualizer = CVScores(\n",
    "        model, cv=cv, size=(1080, 720)\n",
    "    )\n",
    "\n",
    "    visualizer.fit(X, y)\n",
    "    visualizer.show()\n",
    "    \n",
    "    \n",
    "    # Perform 80/20 training/test split\n",
    "    X_train, X_test, y_train, y_test = tts(\n",
    "        X, y, test_size=0.20, random_state=42\n",
    "    )\n",
    "    \n",
    "    expected =  y_test\n",
    "    predicted = model.predict(X_test)\n",
    "\n",
    "    # Compute and return F1 (harmonic mean of precision and recall)\n",
    "    print(\"{}: {}\".format(estimator.__class__.__name__, f1_score(expected, predicted, average='micro')))\n",
    "\n",
    "       \n",
    "    return save_estimator(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    F1_Cross_Validated_Score(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Summary\n",
    "ExtraTrees Classifier yeilds a slight better F1 score and is noteably faster to run over bagging.\n",
    "The precision and recall are slightly better for ExtraTrees.  Med to High fires score the lowest.\n",
    "Educated guess is the more extreme high and low fires are easier to predict than the ones clumped in the middle.\n",
    "The report recommends using ExtraTrees as our final Estimator.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the lastest estimator and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
